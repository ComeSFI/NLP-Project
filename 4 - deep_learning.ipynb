{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "Due to time constraint we are gonna try 2 deep learning models\n",
    "\n",
    "LTSM because LSTMs are particularly effective for tasks involving sequential or time-series data, such as natural language processing (NLP) and speech recognition. They can capture long-term dependencies in sequences.\n",
    "\n",
    "And BERT because BERT (Bidirectional Encoder Representations from Transformers) is effective in NLP because it captures contextual information from both sides of a word in a sentence, uses pre-training on large text corpora for general-purpose understanding, provides contextual embeddings for nuanced meaning, supports transfer learning for specific tasks, has achieved state-of-the-art performance on various benchmarks, employs the efficient Transformer architecture, offers open-source implementations and pre-trained models, and handles multiple languages well.\n",
    "\n",
    "BERT was very long to run so it will be completed only if completly run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoreSentiment\n",
      "POSITIVE    8157\n",
      "NEGATIVE    4069\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('data/x_train.csv')\n",
    "y = pd.read_csv('data/y_train.csv')\n",
    "X=X['text_lemmatized'][:13000]\n",
    "y=y['scoreSentiment'][:13000]\n",
    "na_indices = X[X.isna()].index  # Assuming you want to drop rows with missing values in X\n",
    "\n",
    "# Drop rows from X and y based on na_indices\n",
    "X = X.drop(na_indices)\n",
    "y = y.drop(na_indices)\n",
    "print(y.value_counts())\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10000], y[:10000], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "4000/4000 [==============================] - 100s 25ms/step - loss: 0.5275 - accuracy: 0.7352\n",
      "Epoch 2/8\n",
      "4000/4000 [==============================] - 101s 25ms/step - loss: 0.3087 - accuracy: 0.8705\n",
      "Epoch 3/8\n",
      "4000/4000 [==============================] - 92s 23ms/step - loss: 0.1708 - accuracy: 0.9331\n",
      "Epoch 4/8\n",
      "4000/4000 [==============================] - 92s 23ms/step - loss: 0.0827 - accuracy: 0.9709\n",
      "Epoch 5/8\n",
      "4000/4000 [==============================] - 103s 26ms/step - loss: 0.0360 - accuracy: 0.9871\n",
      "Epoch 6/8\n",
      "4000/4000 [==============================] - 99s 25ms/step - loss: 0.0190 - accuracy: 0.9930\n",
      "Epoch 7/8\n",
      "4000/4000 [==============================] - 98s 24ms/step - loss: 0.0139 - accuracy: 0.9945\n",
      "Epoch 8/8\n",
      "4000/4000 [==============================] - 99s 25ms/step - loss: 0.0099 - accuracy: 0.9969\n",
      "63/63 [==============================] - 1s 13ms/step\n",
      "LSTM Model Accuracy: 0.7575\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test= to_categorical(y_test)\n",
    "\n",
    "def preprocess_text(X_train, max_words, max_sequence_length):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    return tokenizer, sequences\n",
    "\n",
    "max_words = 10000 \n",
    "max_sequence_length = 100 \n",
    "embedding_dim = 32\n",
    "lstm_units = 32\n",
    "\n",
    "tokenizer, X_train= preprocess_text(X_train, max_words, max_sequence_length)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_sequence_length)\n",
    "\n",
    "modelLSTM = keras.Sequential()\n",
    "modelLSTM.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "modelLSTM.add(LSTM(units=lstm_units))\n",
    "modelLSTM.add(Dense(2, activation='softmax'))  # Update output layer to have two nodes\n",
    "modelLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "modelLSTM.fit(X_train, y_train, epochs=8, batch_size=2, callbacks=[early_stopping])\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(np.argmax(y, axis=1), y_pred)\n",
    "    return accuracy\n",
    "\n",
    "lstm_accuracy = evaluate_model(modelLSTM, X_test, y_test)\n",
    "print(\"LSTM Model Accuracy:\", lstm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our accuracy is very good but i cannot manage to plot a confusion amtrix and because of the shape of ytest and ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model : Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\comes\\Documents\\EPF\\Data\\Cours\\5A\\NLP\\NLP-Project\\4 - deep_learning.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/comes/Documents/EPF/Data/Cours/5A/NLP/NLP-Project/4%20-%20deep_learning.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m X_test_tokens \u001b[39m=\u001b[39m tokenizer(X_test_texts, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39mmax_sequence_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/comes/Documents/EPF/Data/Cours/5A/NLP/NLP-Project/4%20-%20deep_learning.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Convert labels to categorical (if not already)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/comes/Documents/EPF/Data/Cours/5A/NLP/NLP-Project/4%20-%20deep_learning.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Ensure that your labels (y_train, y_val, y_test) contain only values 0 or 1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/comes/Documents/EPF/Data/Cours/5A/NLP/NLP-Project/4%20-%20deep_learning.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m y_train_categorical \u001b[39m=\u001b[39m to_categorical(y_train, num_classes\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/comes/Documents/EPF/Data/Cours/5A/NLP/NLP-Project/4%20-%20deep_learning.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_val_categorical \u001b[39m=\u001b[39m to_categorical(y_val, num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/comes/Documents/EPF/Data/Cours/5A/NLP/NLP-Project/4%20-%20deep_learning.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m y_test_categorical \u001b[39m=\u001b[39m to_categorical(y_test, num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\np_utils.py:74\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m n \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     73\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n, num_classes), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m---> 74\u001b[0m categorical[np\u001b[39m.\u001b[39;49marange(n), y] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[39m=\u001b[39m input_shape \u001b[39m+\u001b[39m (num_classes,)\n\u001b[0;32m     76\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X, y are your features and labels\n",
    "# Also, adjust these parameters based on your specific dataset and requirements\n",
    "max_words = 10000 \n",
    "max_sequence_length = 100 \n",
    "embedding_dim = 32\n",
    "lstm_units = 32\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10000], y[:10000], test_size=0.2, random_state=42)\n",
    "X_val = X[10001:12000]\n",
    "y_val = y[10001:12000]\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert X_train, X_val, and X_test to lists of strings\n",
    "X_train_texts = list(X_train)\n",
    "X_val_texts = list(X_val)\n",
    "X_test_texts = list(X_test)\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "X_train_tokens = tokenizer(X_train_texts, padding=True, truncation=True, return_tensors='tf', max_length=max_sequence_length)\n",
    "X_val_tokens = tokenizer(X_val_texts, padding=True, truncation=True, return_tensors='tf', max_length=max_sequence_length)\n",
    "X_test_tokens = tokenizer(X_test_texts, padding=True, truncation=True, return_tensors='tf', max_length=max_sequence_length)\n",
    "\n",
    "# Convert labels to categorical (if not already)\n",
    "# Ensure that your labels (y_train, y_val, y_test) contain only values 0 or 1\n",
    "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=2)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=2e-5)\n",
    "bert_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert BatchEncoding objects to tuples for hashability\n",
    "X_train_tokens_hashable = (\n",
    "    X_train_tokens['input_ids'],\n",
    "    X_train_tokens['token_type_ids'],\n",
    "    X_train_tokens['attention_mask']\n",
    ")\n",
    "\n",
    "X_val_tokens_hashable = (\n",
    "    X_val_tokens['input_ids'],\n",
    "    X_val_tokens['token_type_ids'],\n",
    "    X_val_tokens['attention_mask']\n",
    ")\n",
    "\n",
    "X_test_tokens_hashable = (\n",
    "    X_test_tokens['input_ids'],\n",
    "    X_test_tokens['token_type_ids'],\n",
    "    X_test_tokens['attention_mask']\n",
    ")\n",
    "\n",
    "# Now use the hashable versions when fitting the model\n",
    "history = bert_model.fit(\n",
    "    X_train_tokens_hashable,\n",
    "    y_train_categorical,\n",
    "    validation_data=(X_val_tokens_hashable, y_val_categorical),\n",
    "    epochs=3,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_proba = bert_model.predict(X_test_tokens)\n",
    "y_pred = tf.argmax(y_pred_proba.logits, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"BERT Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
